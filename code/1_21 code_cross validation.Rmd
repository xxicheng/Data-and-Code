---
title: "Cross Validation"
author: "Philip Waggoner, MACS 30100 <br /> University of Chicago"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

# Resampling

```{r}
library(tidyverse)
library(ISLR)
library(broom)
library(rsample)
library(rcfss)
library(yardstick)

Auto <- as_tibble(Auto)

set.seed(1234)

auto_split <- initial_split(data = Auto, 
                            prop = 0.5)

auto_train <- training(auto_split)
auto_test <- testing(auto_split)
```


```{r}
auto_lm <- glm(mpg ~ horsepower, data = auto_train); summary(auto_lm)
```

MSE for training

```{r}
(train_mse <- augment(auto_lm, newdata = auto_train) %>%
  mse(truth = mpg, estimate = .fitted))
```

MSE for validation

```{r}
(test_mse <- augment(auto_lm, newdata = auto_test) %>%
  mse(truth = mpg, estimate = .fitted))
```

Some different models. 

```{r}
# visualize each model
ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(alpha = .1) +
  geom_smooth(aes(color = "1"),
              method = "glm",
              formula = y ~ poly(x, i = 1, raw = TRUE),
              se = FALSE) +
  geom_smooth(aes(color = "2"),
              method = "glm",
              formula = y ~ poly(x, i = 2, raw = TRUE),
              se = FALSE) +
  geom_smooth(aes(color = "3"),
              method = "glm",
              formula = y ~ poly(x, i = 3, raw = TRUE),
              se = FALSE) +
  geom_smooth(aes(color = "4"),
              method = "glm",
              formula = y ~ poly(x, i = 4, raw = TRUE),
              se = FALSE) +
  geom_smooth(aes(color = "5"),
              method = "glm",
              formula = y ~ poly(x, i = 5, raw = TRUE),
              se = FALSE) +
  scale_color_brewer(type = "qual", palette = "Dark2") +
  labs(x = "Horsepower",
       y = "MPG",
       color = "Polynomial\norder") +
  theme_minimal()

# train and eval simultaneously
poly_results <- function(train, test, i) {
  mod <- glm(mpg ~ poly(horsepower, i, raw = TRUE), 
             data = train)
  res <- augment(mod, 
                 newdata = test) %>%
    mse(truth = mpg, estimate = .fitted)
  res
}

library(magrittr)

poly_mse <- function(i, train, test){
  poly_results(train, test, i) %$%
    mean(.estimate)
}

cv_mse <- tibble(terms = seq(from = 1, to = 5),
                 mse_test = map_dbl(terms, poly_mse, auto_train, auto_test))

ggplot(cv_mse, aes(terms, mse_test)) +
  geom_line() +
  labs(title = "Evaluating quadratic linear models",
       subtitle = "Using validation set",
       x = "Highest-order polynomial",
       y = "Mean Squared Error") +
  theme_minimal()
```

## Classification

A case study with classification.

```{r}
library(titanic)

titanic <- as_tibble(titanic_train) %>%
  mutate(Survived = factor(Survived))

titanic %>%
  head(n = 5)
```

```{r}
survive_age_woman_x <- glm(Survived ~ Age * Sex, data = titanic,
                           family = binomial)
summary(survive_age_woman_x)
```

```{r}
# helper fun
logit2prob <- function(x){
  exp(x) / (1 + exp(x))
}
```

```{r}
# split the data into training and validation sets
titanic_split <- initial_split(data = titanic, 
                               prop = 0.5)

# fit model to training data
train_model <- glm(Survived ~ Age * Sex, 
                   data = training(titanic_split), # note the different syntax used here
                   family = binomial); broom::tidy(train_model)

# calculate predictions using validation set
x_test_accuracy <- augment(train_model, 
                           newdata = testing(titanic_split)) %>% 
  as_tibble() %>%
  mutate(.prob = logit2prob(.fitted),
         .pred = factor(round(.prob)))

# calculate test accuracy rate
accuracy(x_test_accuracy, 
         truth = Survived, 
         estimate = .pred)
```


# LOOCV

## LOOCV in regression

```{r}
library(tidyverse)
library(ISLR)
library(broom)
library(rsample)
library(rcfss)
library(yardstick)

loocv_data <- loo_cv(Auto)

loocv_data %>% 
  names()
```

Note: each element of `loocv_data$splits` is an object of class `rsplit`. 

```{r}
first_resample <- loocv_data$splits[[1]]
first_resample
```


```{r}
# but first, note training() and analysis() from resample contain same information; just different forms
first_resample %>% 
  analysis() %>% # data frame version 
  head(n = 5)

first_resample %>% 
  training() %>% 
  head(n = 5)

# same with assessment() and testing()
first_resample %>% 
  assessment() %>% 
  head(n = 5)

first_resample %>% 
  testing() %>% 
  head(n = 5)
```


```{r}
holdout_results <- function(splits) {
  mod <- glm(mpg ~ horsepower, 
             data = analysis(splits))
  res <- augment(mod, newdata = assessment(splits)) %>%
    mse(truth = mpg, estimate = .fitted)
  res
}
```

This function works also for a single resample:

```{r}
holdout_results(loocv_data$splits[[1]])
```


```{r}
loocv_data_poly1 <- loocv_data %>%
  mutate(results = map(splits, holdout_results)) %>%
  unnest(results) %>%
  spread(.metric, .estimate)
loocv_data_poly1 %>% 
  head(n = 5)

loocv_data_poly1 %>%
  summarize(mse = mean(mse))
```


```{r}
# modified function to estimate model with varying polynomial order
holdout_results <- function(splits, i) {
  mod <- glm(mpg ~ poly(horsepower, i, raw = TRUE), 
             data = analysis(splits))
  
  res <- augment(mod, newdata = assessment(splits)) %>%
    mse(truth = mpg, estimate = .fitted)
  res
}

# function to return MSE for a specific polynomial term
poly_mse <- function(i, loocv_data){
  loocv_mod <- loocv_data %>%
    mutate(results = map(splits, holdout_results, i)) %>%
    unnest(results) %>%
    spread(.metric, .estimate)
  
  mean(loocv_mod$mse)
}

library(tictoc)

{ # wrap and time
tic()
cv_mse <- tibble(terms = seq(from = 1, to = 5),
                 mse_loocv = map_dbl(terms, poly_mse, loocv_data))
toc() 
} # takes ~25 seconds on my slower computer

cv_mse

ggplot(cv_mse, aes(terms, mse_loocv)) +
  geom_line() +
  labs(title = "Comparing quadratic linear models",
       subtitle = "Using LOOCV",
       x = "HPolynomial\norder",
       y = "Mean Squared Error") +
  theme_minimal()
```

## LOOCV in classification

Let's verify the error rate of our interactive terms model for the Titanic data set:

```{r}
titanic <- as_tibble(titanic_train) %>%
  mutate(Survived = factor(Survived))

holdout_results <- function(splits) {
  mod <- glm(Survived ~ Age * Sex, data = analysis(splits),
             family = binomial)
  res <- augment(mod, newdata = assessment(splits)) %>% 
    as_tibble() %>%
    mutate(.prob = logit2prob(.fitted),
           .pred = round(.prob))
  res
}

titanic_loocv <- loo_cv(titanic) %>%
  mutate(results = map(splits, holdout_results)) %>%
  unnest(results) %>%
  mutate(.pred = factor(.pred)) %>%
  group_by(id) %>%
  accuracy(truth = Survived, estimate = .pred)

1 - mean(titanic_loocv$.estimate, na.rm = TRUE)
```


# k-fold CV

Let's go back to the `Auto` data set and comparing across polynomial orders.

```{r}
# helper fun
holdout_results <- function(splits, i) {
  mod <- glm(mpg ~ poly(horsepower, i, raw = TRUE), data = analysis(splits))
  
  holdout <- assessment(splits)
  
  res <- augment(mod, newdata = holdout) %>%
    mse(truth = mpg, estimate = .fitted)

  res
}

# function to return MSE for a specific fit
poly_mse <- function(i, vfold_data){
  vfold_mod <- vfold_data %>%
    mutate(results = map(splits, holdout_results, i)) %>%
    unnest(results) %>%
    spread(.metric, .estimate)
  
  mean(vfold_mod$mse)
}


auto_cv10 <- vfold_cv(data = Auto, 
                      v = 10)

# as before...
auto_cv10 %>% 
  names()

cv_mse <- tibble(terms = seq(from = 1, 
                             to = 5),
                 mse_vfold = map_dbl(terms, poly_mse, auto_cv10))
cv_mse
```

Comparison?

```{r}
auto_loocv <- loo_cv(Auto)

tibble(terms = seq(from = 1, to = 5), # takes a few seconds, given the mapping
       `10-fold` = map_dbl(terms, poly_mse, auto_cv10),
       LOOCV = map_dbl(terms, poly_mse, auto_loocv)
) %>%
  gather(method, MSE, -terms) %>%
  ggplot(aes(terms, MSE, color = method)) +
  geom_line() +
  labs(title = "MSE estimates",
       subtitle = "Comparing model complexity and accuracy",
       x = "Degree of Polynomial",
       y = "Mean Squared Error",
       color = "CV\nMethod") + 
  theme_minimal()
```

# On your own

For this section, you will work in small groups of 4-5. *I will create these groups at random*. 

**IMPORTANT**: _Don't forget that this code you're working on here is due at the appropriate Canvas module (in the form of an attachment to a "Discussion" post) prior to 5:00 pm CDT tomorrow. You need only submit a **single** file/script to be considered for credit (i.e., this .Rmd with your code inserted below each question). Recall, I don't care whether you got things right. I only care that attempts to each question have been made._ 

Return to the Titanic data, and apply 10-fold cross validation to a classification task. As noted before, though we haven't covered classification yet, you should have seen enough in today's session to complete (or at least attempt) the following. I will get you started with the packages and data needed to respond to each prompt below.

```{r}
library(tidyverse)
library(tidymodels)
library(titanic)

titanic <- as_tibble(titanic_train) %>%
  mutate(Survived = factor(Survived))
```

1. Use 10-fold cross validation to build and evaluate a logistic regression predicting `Survived` as a function of interacting `Age` and `Sex`. To answer this, you will need to:

    - Build the classifer on the training set(s) across folds
    - Evaluate each classifer using the test set(s) across folds

```{r}
titanic <- as_tibble(titanic_train) %>%
  mutate(Survived = factor(Survived))

holdout_results <- function(splits) {
  mod <- glm(Survived ~ Age * Sex, data = analysis(splits),
             family = binomial)
  res <- augment(mod, newdata = assessment(splits)) %>% 
    as_tibble() %>%
    mutate(.prob = logit2prob(.fitted),
           .pred = round(.prob))
  res
}

titanic_10f <- vfold_cv(data = titanic, v = 10) %>%
  mutate(results = map(splits, holdout_results)) %>%
  unnest(results) %>%
  mutate(.pred = factor(.pred)) %>%
  group_by(id) %>%
  accuracy(truth = Survived, estimate = .pred)

titanic_10f
```

2. Calculate the cross-validation *error rate* (not the accuracy rate) from your solution and report it. 
```{r}
1 - mean(titanic_10f$.estimate, na.rm = TRUE)
```

3. Is this similar to the error from using LOOCV earlier in the session? Why or why not, do you think? (offer just a couple thoughts on the patterns -- differences and similarities in error, computational speed, etc. -- from each approach to resampling in this classification setting).

```{r}
titanic <- as_tibble(titanic_train) %>%
  mutate(Survived = factor(Survived))

holdout_results <- function(splits) {
  mod <- glm(Survived ~ Age * Sex, data = analysis(splits),
             family = binomial)
  res <- augment(mod, newdata = assessment(splits)) %>% 
    as_tibble() %>%
    mutate(.prob = logit2prob(.fitted),
           .pred = round(.prob))
  res
}

titanic_loocv <- loo_cv(titanic) %>%
  mutate(results = map(splits, holdout_results)) %>%
  unnest(results) %>%
  mutate(.pred = factor(.pred)) %>%
  group_by(id) %>%
  accuracy(truth = Survived, estimate = .pred)

1 - mean(titanic_loocv$.estimate, na.rm = TRUE)
```
Yes it is. 10-fold cross validation is a bit smaller since the sample number is smaller.
